{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Process of Latent Aspect Rating Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Define Review Class\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk.data\n",
    "\n",
    "with open( 'stopwords.dat', 'r' ) as stopfile:\n",
    "    en_stop = stopfile.readlines()\n",
    "    en_stop = [ item.split('\\n')[0] for item in en_stop if not item in ['\\n'] ]\n",
    "\n",
    "class Review:\n",
    "    # Initialization\n",
    "    def __init__(self, hotel='', title='', rating='', aspects='', content='' ):\n",
    "        self.hotel = hotel\n",
    "        self.title = title\n",
    "        self.rating = rating\n",
    "        self.aspects = aspects\n",
    "        self.content = content\n",
    "        \n",
    "    def Sentences( self ):\n",
    "        tokenizer = nltk.data.load( 'tokenizers/punkt/english.pickle' )\n",
    "        self.sentences = tokenizer.tokenize( self.title.lower() + self.content.lower() )\n",
    "        \n",
    "    def pre_process( self ):\n",
    "        tokenizer = RegexpTokenizer( r'\\w+' )\n",
    "        temp = []\n",
    "        for sentence in self.sentences:\n",
    "            tokens = tokenizer.tokenize( sentence )\n",
    "            tokens_nostop = [ item for item in tokens if not unicode( item ) in en_stop ]\n",
    "            temp.append( tokens_nostop )\n",
    "        self.tokens = temp\n",
    "        \n",
    "    def removeTerms( self, Vocabulary ):\n",
    "        terms_removed = []\n",
    "        for sentence in self.tokens:\n",
    "            new_tokens = [ item for item in sentence if  item in Vocabulary ]\n",
    "            terms_removed.append( new_tokens )\n",
    "        self.terms_removed = terms_removed\n",
    "        \n",
    "    def Stemmer( self ):\n",
    "        p_stemmer = PorterStemmer()\n",
    "        sentences_final = []\n",
    "        for sentence in self.terms_removed:\n",
    "            sentence = [ p_stemmer.stem( item ) for item in sentence ]\n",
    "            sentences_final.append( sentence )\n",
    "        self.sentences_final = sentences_final\n",
    "        \n",
    "    def Lemmatizer( self ):\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        sentences_final = []\n",
    "        for sentence in self.terms_removed:\n",
    "            sentence = [ lemmatizer.lemmatize( item ) for item in sentence ]\n",
    "            sentences_final.append( sentence )\n",
    "        self.sentences_final = sentences_final       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tripadvisor loaded.\n"
     ]
    }
   ],
   "source": [
    "## Load TripAdvisor Reviews which includes Overall Rating, Aspect Rating and review contents\n",
    "\n",
    "import csv\n",
    "\n",
    "TripAdvisor = []\n",
    "with open( 'tripadvisor_reviews.csv' , 'r' ) as csvfile:\n",
    "    Reviewreader = csv.reader( csvfile, delimiter=',' )\n",
    "    for row in Reviewreader:\n",
    "        hotel = row[1].decode('utf-8')\n",
    "        title = row[2].decode('utf-8')\n",
    "        rating = float( row[5] ) \n",
    "        aspects = row[6].decode('utf-8')\n",
    "        content = row[7].decode('utf-8')\n",
    "        TripAdvisor.append( Review( hotel, title, rating, aspects, content ) )\n",
    "    print 'Tripadvisor loaded.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get Preprocessing for all the reviews\n",
    "i = 0\n",
    "for idx in range( len( TripAdvisor ) ):\n",
    "    TripAdvisor[ idx ].Sentences()\n",
    "    TripAdvisor[ idx ].pre_process()\n",
    "    i += 1\n",
    "    if i%10000 == 0:\n",
    "        print i, 'reviews have been processed.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Count the frequency of each token\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "Tokens = []\n",
    "for review in TripAdvisor:\n",
    "    for tokens in review.tokens:\n",
    "        Tokens += tokens\n",
    "        \n",
    "TermList = Counter( Tokens )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get terms occuring less than 5 in the corpus\n",
    "RemoveTermList = []\n",
    "for token in Tokens:\n",
    "    if TermList[ token ] < 5:\n",
    "        RemoveTermList.append( token )\n",
    "\n",
    "RemoveTermList = list( set( RemoveTermList ) )\n",
    "Tokens = list( set( Tokens ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158722 118896\n",
      "39826\n"
     ]
    }
   ],
   "source": [
    "# Build the vocabulary for the corpus\n",
    "Vocabulary = []\n",
    "i = 0\n",
    "print len( Tokens ), len( RemoveTermList )\n",
    "\n",
    "Vocabulary =  list( set( Tokens ).difference( set( RemoveTermList ) ) )\n",
    "print len( Vocabulary )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove terms appearing less than 5 times\n",
    "i=0\n",
    "for idx in range( len( TripAdvisor ) ):\n",
    "    TripAdvisor[ idx ].removeTerms( Vocabulary )\n",
    "    i += 1\n",
    "    if i%10000 == 0:\n",
    "        print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Stemming each word to its root with Porter Stemmer\n",
    "i = 0\n",
    "for idx in range( len( TripAdvisor ) ):\n",
    "    TripAdvisor[ idx ].Stemmer()\n",
    "    i += 1\n",
    "    if i%10000 == 0:\n",
    "        print i\n",
    "        \n",
    "# Preprocessing done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Saving and Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save Variables into file\n",
    "import pickle\n",
    "\n",
    "with open( 'TripAdvisorReviews.pickle', 'w' ) as TripAdvisorfile:\n",
    "    pickle.dump( [ TripAdvisor ] , TripAdvisorfile )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open( 'Vocabulary.pickle', 'w' ) as Vfile:\n",
    "    pickle.dump( [Vocabulary], Vfile )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load saved processed reviews and vocabulary\n",
    "import pickle\n",
    "\n",
    "with open( 'TripAdvisorReviews.pickle', 'r' ) as TripAdvisorfile:\n",
    "    [ TripAdvisor ] = pickle.load( TripAdvisorfile )\n",
    "    \n",
    "with open( 'Vocabulary.pickle', 'r' ) as Vfile:\n",
    "    [ Vocabulary ] = pickle.load( Vfile )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace Stemming with Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def Lemmatizer( self ):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    sentences_final = []\n",
    "    for sentence in self.terms_removed:\n",
    "        sentence = [ lemmatizer.lemmatize( item ) for item in sentence ]\n",
    "        sentences_final.append( sentence )\n",
    "    return sentences_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "220000\n",
      "230000\n",
      "240000\n",
      "250000\n",
      "260000\n",
      "270000\n",
      "280000\n",
      "290000\n",
      "300000\n",
      "310000\n",
      "320000\n",
      "330000\n",
      "340000\n",
      "350000\n",
      "360000\n",
      "370000\n",
      "380000\n",
      "390000\n",
      "400000\n",
      "410000\n",
      "420000\n",
      "430000\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "for idx in range( len( TripAdvisor ) ):\n",
    "    TripAdvisor[ idx ].final_sentences_final = Lemmatizer( TripAdvisor[ idx ] )\n",
    "    i += 1\n",
    "    if i%10000 == 0:\n",
    "        print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open( 'Vocabulary.pickle', 'r' ) as Vfile:\n",
    "    [ Vocabulary ] = pickle.load( Vfile )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TripSentences_final = [ review.sentences_final for review in TripAdvisor ]\n",
    "\n",
    "with open( 'review_sentences.pickle', 'w') as sentence_file:\n",
    "    pickle.dump( [ TripSentences_final ], sentence_file )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Small Data Collection -- Sample 20000 reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open( 'ReviewToSentences.pickle', 'r' ) as sentencefile:\n",
    "    [ ReviewToSentences ] = pickle.load( sentencefile )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "SampleSize = 2000\n",
    "\n",
    "SampledIdx = random.sample( xrange( len( ReviewToSentences ) ) , SampleSize )\n",
    "SampledSentences = []\n",
    "\n",
    "for idx in SampledIdx:\n",
    "    ReviewSentences = [ Sentences[i] for i in ReviewToSentences[ idx ] ]\n",
    "    SampledSentences += ReviewSentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open( 'SampledData-2000.pickle', 'r' ) as savefile:\n",
    "    SampledIdx, Sentences = pickle.load( savefile )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'stay', u'sound', u'sleeper']\n"
     ]
    }
   ],
   "source": [
    "print Sentences[90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SampledSentences = []\n",
    "for idx in SampledIdx:\n",
    "    ReviewSentences = TripAdvisor[ idx ].final_sentences_final\n",
    "    SampledSentences += ReviewSentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open( 'SampledData-2000.pickle', 'w' ) as savefile:\n",
    "    pickle.dump( [ SampledIdx, SampledSentences ] , savefile )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aspect Segmentation -- Bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<value> ['value', 'price', 'quality', 'worth']\n",
      "<room> ['room', 'suite', 'view', 'bed']\n",
      "<location> ['location', 'traffic', 'minute', 'restaurant']\n",
      "<cleanliness> ['clean', 'dirty', 'maintain', 'smell']\n",
      "<front desk> ['stuff', 'check', 'help', 'reservation']\n",
      "<service> ['service', 'food', 'breakfast', 'buffet']\n",
      "<business service> ['business', 'center', 'computer', 'internet']\n"
     ]
    }
   ],
   "source": [
    "# Load seed Topic List\n",
    "import csv\n",
    "\n",
    "To = {}\n",
    "with open( 'hotel_bootstrapping.dat', 'r' ) as seedfile:\n",
    "    Reader = csv.reader( seedfile, delimiter='\\t' )\n",
    "    for row in Reader:\n",
    "        To[ row[0].decode('utf-8') ] = row[1:]\n",
    "        print row[0], row[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open( 'review_sentences.pickle', 'r' ) as sentence_file:\n",
    "    [ TripSentences_final ] = pickle.load( sentence_file )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Sentences = []\n",
    "ReviewToSentences = []\n",
    "\n",
    "index = 0\n",
    "for idx in range( len( TripSentences_final ) ):\n",
    "    reviewTosentence = []\n",
    "    for sentence in TripSentences_final[ idx ]:\n",
    "        Sentences.append( sentence )\n",
    "        reviewTosentence.append( index )\n",
    "        index += 1\n",
    "    ReviewToSentences.append( reviewTosentence )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open( 'ReviewToSentences.pickle', 'w' ) as sentencefile:\n",
    "    pickle.dump( [ ReviewToSentences ] , sentencefile )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open( 'sentences.pickle', 'w' ) as sentencefile:\n",
    "    pickle.dump( [ Sentences ] , sentencefile )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 begins:\n",
      "Aspect Annotation Done.\n",
      "0\n",
      "1 1000 <service>\n",
      "1 2000 <service>\n",
      "1 3000 <service>\n",
      "1 4000 <service>\n",
      "1 5000 <service>\n",
      "1 6000 <service>\n",
      "1 7000 <service>\n",
      "1 8000 <service>\n",
      "1 9000 <service>\n",
      "1 10000 <service>\n",
      "10000\n",
      "1 11000 <service>\n",
      "1 12000 <service>\n",
      "1 13000 <service>\n",
      "1 14000 <service>\n",
      "1 15000 <service>\n",
      "1 16000 <service>\n",
      "1 17000 <service>\n",
      "1 18000 <service>\n",
      "1 19000 <service>\n",
      "1 20000 <service>\n",
      "20000\n",
      "1 21000 <service>\n",
      "1 22000 <service>\n",
      "1 23000 <service>\n",
      "1 24000 <service>\n",
      "1 25000 <service>\n",
      "1 26000 <service>\n",
      "1 27000 <service>\n",
      "1 28000 <service>\n",
      "1 29000 <service>\n",
      "1 30000 <service>\n",
      "30000\n",
      "1 31000 <service>\n",
      "1 32000 <service>\n",
      "1 33000 <service>\n",
      "1 34000 <service>\n",
      "1 35000 <service>\n",
      "1 36000 <service>\n",
      "1 37000 <service>\n",
      "1 38000 <service>\n",
      "1 39000 <service>\n",
      "0\n",
      "1 1000 <value>\n",
      "1 2000 <value>\n",
      "1 3000 <value>\n",
      "1 4000 <value>\n",
      "1 5000 <value>\n",
      "1 6000 <value>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-71bdc9f16ee8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     57\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mSentence_aspects\u001b[0m\u001b[1;33m[\u001b[0m \u001b[0midx\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m                     \u001b[0mC_1\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mword\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m                         \u001b[0mC_3\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#with open( 'sentences.pickle', 'r' ) as sentence_file:\n",
    "#    [ Sentences ] = pickle.load( sentence_file )\n",
    "\n",
    "Sentences = SampledSentences\n",
    "    \n",
    "with open( 'Vocabulary.pickle', 'r' ) as Vfile:\n",
    "    [ Vocabulary ] = pickle.load( Vfile )\n",
    "\n",
    "p = 5  # selection threshold\n",
    "max_iter = 10 # iteration step limit\n",
    "tf_cut = 10 # Term frequency filtering\n",
    "\n",
    "iter_num = 0 # Initialize iter_num and keyword list changed.\n",
    "Flag = True\n",
    "\n",
    "Progress = open('RunningProgress.csv', 'a')\n",
    "\n",
    "ProWriter = csv.writer( Progress , delimiter=',' )\n",
    "\n",
    "while iter_num < max_iter and Flag:\n",
    "    Flag = False # set keyword list to unchanged\n",
    "    iter_num += 1\n",
    "    print 'Iteration', iter_num, 'begins:'\n",
    "    \n",
    "    # Match the aspect keywords in each sentence and record its matching\n",
    "    Sentence_aspects = []\n",
    "    for sentence in Sentences:\n",
    "        aspectcount = {}\n",
    "        aspects = [] # aspects of this sentence\n",
    "        countMax = 0 # maximum count(i)\n",
    "        for key in T.keys():\n",
    "            count = 0\n",
    "            for keyword in T[ key ]:\n",
    "                count += sentence.count( keyword )\n",
    "            aspectcount[ key ] = count\n",
    "            if count > countMax:\n",
    "                countMax = count\n",
    "                    \n",
    "        # Assign aspect i\n",
    "        for key in T.keys():\n",
    "            if aspectcount[ key ] == countMax:\n",
    "                aspects.append( key )\n",
    "            \n",
    "        Sentence_aspects.append( aspects )\n",
    "                   \n",
    "    # sentence annotation with aspect assignment done\n",
    "    print 'Aspect Annotation Done.'\n",
    "    \n",
    "    \n",
    "    for key in T.keys():\n",
    "        chi_list = []\n",
    "        count = 0\n",
    "        for j, word in enumerate( Vocabulary ):\n",
    "            C_1, C_2, C_3, C_4, C = 0, 0, 0, 0, 0\n",
    "            for idx,sentence in enumerate( Sentences ):\n",
    "                C += sentence.count( word )\n",
    "                if key in Sentence_aspects[ idx ]:\n",
    "                    C_1 += sentence.count( word )\n",
    "                    if not word in sentence:\n",
    "                        C_3 += 1.0\n",
    "                else:\n",
    "                    C_2 += sentence.count( word )\n",
    "                    if not word in sentence:\n",
    "                        C_4 += 1.0\n",
    "            if j%10000 == 0:\n",
    "                print j\n",
    "            # Calculate the chi for each word\n",
    "            denominator = (C_1 + C_3) * (C_2 + C_4) * (C_1 + C_2) * (C_3 + C_4)\n",
    "            nominator = C * ( C_1*C_4 - C_2*C_3 ) * ( C_1*C_4 - C_2*C_3 )\n",
    "            if denominator > 0 and C_1 + C_2 > tf_cut:\n",
    "                chi = nominator / float( denominator )\n",
    "            else:\n",
    "                chi = 0.0\n",
    "            chi_list.append( ( word, chi ) )\n",
    "            count += 1\n",
    "            \n",
    "            if count % 1000 == 0:\n",
    "                print iter_num,  count, key\n",
    "                \n",
    "        Chi_sorted = sorted( chi_list , key=lambda tup: tup[1] , reverse = True )\n",
    "    \n",
    "        # Joint top p words into aspect keywords list\n",
    "        len_original = len( T[ key ] )\n",
    "        for idx in range( p ):\n",
    "            T[ key ].append( Chi_sorted[ idx ][0] )\n",
    "        T[ key ] = list( set( T[ key ] ) )\n",
    "    \n",
    "        # Test whether the aspect keyword list has changed.\n",
    "        if len( T[ key ] ) > len_original:\n",
    "            Flag = True\n",
    "            \n",
    "Progress.close()\n",
    "            \n",
    "with open('SentenceAspects_Tkey.pickle', 'w') as resultfile:\n",
    "    pickle.dump( [ TripSentence_aspects, T ], resultfile )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Sampled Data and Mapping from review to sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load saved processed reviews and vocabulary\n",
    "import pickle\n",
    "\n",
    "with open( 'TripAdvisorReviews.pickle', 'r' ) as TripAdvisorfile:\n",
    "    [ TripAdvisor ] = pickle.load( TripAdvisorfile )\n",
    "    \n",
    "with open( 'Vocabulary.pickle', 'r' ) as Vfile:\n",
    "    [ Vocabulary ] = pickle.load( Vfile )\n",
    "    \n",
    "with open( 'SampledData-2000.pickle', 'r' ) as savefile:\n",
    "    SampledIdx, Sentences = pickle.load( savefile )\n",
    "    \n",
    "with open( 'ReviewToSentences.pickle', 'r' ) as sentencefile:\n",
    "    [ ReviewToSentences ] = pickle.load( sentencefile )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿<value> ['ranges', 'half', 'accomodate', 'inclusive', 'deal', 'value', 'paying', 'worth', '!', 'extra', '$', 'pricy', 'overprice', '%', 'atmosphere', 'bargain', 'cost', 'charged', 'discount', 'fee', 'hotwire', 'price', 'vacation', 'tax', 'cash', 'quality', 'penny', 'low', 'cheap', 'bill', 'range', 'expectation', 'choice', 'usd', 'expensive', 'pesos', 'barter', 'priceline', 'honeymoon', 'paid', 'accomodation', 'rate', 'accommodate', 'money', 'pay', 'dollar', 'experience', 'resort', 'cheaper', 'definitely', 'anniversary', 'fraction', 'negotiate', 'dollars', 'star', 'quoted', 'rates', 'haggle', 'accommodation', 'prices', 'rating', 'taxes']\n",
      "<room> ['door', 'carpet', 'house', 'furniture', 'bathroom', 'ready', 'comfortable', 'decor', 'furnish', 'open', 'hear', 'renovation', 'light', 'rooms', 'screen', 'mansion', 'condition', 'bathtub', 'bed', 'sink', 'windows', 'kitchen', 'quiet', 'conditioner', 'sleep', 'conditioning', 'double', 'bedroom', 'shampoo', 'size', 'upgrade', 'huge', 'beds', 'floor', 'window', 'tub', 'shower', 'air', 'soap', 'view', 'suite', 'small', 'wall', 'large', 'room', 'pillow', 'television', 'housekeeping', 'queen', 'apartment', 'space', 'king', 'modern', 'mirror', 'twin', 'overlook', 'spacious', 'louver', 'toilet', 'bath', 'toiletry', 'balcony', 'decorate', 'stay', 'noise', 'chair', 'tower', 'tv', 'book', 'suit', 'facing', 'hairdryer', 'night', 'square', 'courtyard', 'channel', 'pillows', 'upgraded']\n",
      "<location> ['shop', 'parking', 'bus', 'centre', 'location', 'street', 'shuttle', 'beach', 'transportation', 'close', 'city', 'center', 'airport', 'sight-see', 'touristy', 'subway', 'terminal', 'shops', 'located', 'train', 'bank', 'conference', 'place', 'pantheon', 'central', 'station', 'tram', 'museum', 'traffic', 'market', 'near', 'avenue', 'shopping', 'underground', 'wharf', 'walk', 'wall', 'outside', 'position', 'car', 'stop', 'supermarket', 'walking', 'taxi', 'tube', 'garage', 'block', 'restaurant', 'surround', 'minute', 'short', 'distance', 'easy', 'transport', 'union', 'opera', 'trip', 'boutique', 'district', 'locate', 'restaurants', 'downtown', 'min', 'boulevard', 'site', 'minutes', 'stay', 'park', 'metro', 'attractions', 'mins', 'blocks', 'bloc', 'convenient', 'great', 'route', 'square', 'plaza', 'mall']\n",
      "<cleanliness> ['maintain', 'bathrooms', 'heated', 'urine', 'barrier', 'grounds', 'tidy', 'attentive', 'white', 'bottled', 'kids', 'impeccably', 'teeth', 'professional', 'pools', 'smoke', 'turquoise', 'immaculately', 'neat', 'spotlessly', 'smoker', 'loungers', 'shade', 'towels', 'linen', 'smell', 'cigarette', 'chairs', 'plenty', 'musty', 'clear', 'clean', 'pool', 'dirty', 'slide', 'crowded', 'nonsmoking', 'pressure', 'crystal', 'exceptionally', 'towel', 'cleanliness', 'bug', 'well-maintained', 'lazy', 'maintained']\n",
      "<service> ['guide', 'highspeed', 'manager', 'bellman', 'e-mail', 'luggage', 'elevator', 'free', 'help', 'drink', 'shelf', 'lugged', 'smile', 'wine', 'serve', 'rude', 'continental', 'calls', 'wi-fi', 'router', 'reservation', 'pc', 'cafe', 'eat', 'bar', 'courteous', 'newspaper', 'buffets', 'computer', 'breakfast', 'carte', 'check-in', 'desk', 'laundry', 'computers', 'massage', 'wired', 'concierge', 'waiter', 'extremely', 'frontdesk', 'connectivity', 'front', 'friendly', 'access', 'greet', 'printer', 'route', 'club', 'broadband', 'polite', 'high-speed', 'hi-speed', 'agressive', 'buffet', 'pcs', 'request', 'lunch', 'gym', 'welcome', 'connection', 'fax', 'property', 'printing', 'emails', 'helpful', 'dinner', 'inform', 'stuff', 'email', 'food', 'gem', 'management', 'checkout', 'internet', 'modem', 'connect', 'fitness', 'garage', 'security', 'restaurant', 'drinks', 'immodium', 'network', 'staff', 'wireless', 'ethernet', 'facility', 'ate', 'lounge', 'lan', 'supply', 'printers', 'office', 'receptionist', 'suitcase', 'speed', 'boarding', 'dial-up', 'print', 'entertainment', 'reception', 'wifi', 'meeting', 'service', 'cord']\n"
     ]
    }
   ],
   "source": [
    "## Directly Apply Keyword list from Hongning Wang's Paper\n",
    "import csv\n",
    "\n",
    "T = {}\n",
    "with open( 'hotel_bootstrapping_May10.dat' , 'r') as KeywordFile:\n",
    "    Reader = csv.reader( KeywordFile, delimiter=' ' )\n",
    "    for row in Reader:\n",
    "        T[ row[0].decode('utf-8') ] = row[1:]\n",
    "        print row[0], row[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 sentence annotation done.\n",
      "Aspect Annotation Done.\n"
     ]
    }
   ],
   "source": [
    "## Aspect Annotation with Above Aspect Keyword list\n",
    "Sentence_aspects = []\n",
    "for idx, sentence in enumerate( Sentences ):\n",
    "    aspectcount = {}\n",
    "    aspects = [] # aspects of this sentence\n",
    "    countMax = 0 # maximum count(i)\n",
    "    for key in T.keys():\n",
    "        count = 0\n",
    "        for keyword in T[ key ]:\n",
    "            count += sentence.count( keyword )\n",
    "        aspectcount[ key ] = count\n",
    "        if count > countMax:\n",
    "            countMax = count\n",
    "                    \n",
    "    # Assign aspect i\n",
    "    for key in T.keys():\n",
    "        if aspectcount[ key ] == countMax:\n",
    "            aspects.append( key )\n",
    "            \n",
    "    Sentence_aspects.append( aspects )\n",
    "    if idx > 0 and idx%10000 == 0:\n",
    "        print idx, 'sentence annotation done.'\n",
    "                   \n",
    "# sentence annotation with aspect assignment done\n",
    "print 'Aspect Annotation Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Corpus Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<service> ['guide', 'highspeed', 'manager', 'bellman', 'e-mail', 'luggage', 'elevator', 'free', 'help', 'drink', 'shelf', 'lugged', 'smile', 'wine', 'serve', 'rude', 'continental', 'calls', 'wi-fi', 'router', 'reservation', 'pc', 'cafe', 'eat', 'bar', 'courteous', 'newspaper', 'buffets', 'computer', 'breakfast', 'carte', 'check-in', 'desk', 'laundry', 'computers', 'massage', 'wired', 'concierge', 'waiter', 'extremely', 'frontdesk', 'connectivity', 'front', 'friendly', 'access', 'greet', 'printer', 'route', 'club', 'broadband', 'polite', 'high-speed', 'hi-speed', 'agressive', 'buffet', 'pcs', 'request', 'lunch', 'gym', 'welcome', 'connection', 'fax', 'property', 'printing', 'emails', 'helpful', 'dinner', 'inform', 'stuff', 'email', 'food', 'gem', 'management', 'checkout', 'internet', 'modem', 'connect', 'fitness', 'garage', 'security', 'restaurant', 'drinks', 'immodium', 'network', 'staff', 'wireless', 'ethernet', 'facility', 'ate', 'lounge', 'lan', 'supply', 'printers', 'office', 'receptionist', 'suitcase', 'speed', 'boarding', 'dial-up', 'print', 'entertainment', 'reception', 'wifi', 'meeting', 'service', 'cord']\n",
      "<location> ['shop', 'parking', 'bus', 'centre', 'location', 'street', 'shuttle', 'beach', 'transportation', 'close', 'city', 'center', 'airport', 'sight-see', 'touristy', 'subway', 'terminal', 'shops', 'located', 'train', 'bank', 'conference', 'place', 'pantheon', 'central', 'station', 'tram', 'museum', 'traffic', 'market', 'near', 'avenue', 'shopping', 'underground', 'wharf', 'walk', 'wall', 'outside', 'position', 'car', 'stop', 'supermarket', 'walking', 'taxi', 'tube', 'garage', 'block', 'restaurant', 'surround', 'minute', 'short', 'distance', 'easy', 'transport', 'union', 'opera', 'trip', 'boutique', 'district', 'locate', 'restaurants', 'downtown', 'min', 'boulevard', 'site', 'minutes', 'stay', 'park', 'metro', 'attractions', 'mins', 'blocks', 'bloc', 'convenient', 'great', 'route', 'square', 'plaza', 'mall']\n",
      "<cleanliness> ['maintain', 'bathrooms', 'heated', 'urine', 'barrier', 'grounds', 'tidy', 'attentive', 'white', 'bottled', 'kids', 'impeccably', 'teeth', 'professional', 'pools', 'smoke', 'turquoise', 'immaculately', 'neat', 'spotlessly', 'smoker', 'loungers', 'shade', 'towels', 'linen', 'smell', 'cigarette', 'chairs', 'plenty', 'musty', 'clear', 'clean', 'pool', 'dirty', 'slide', 'crowded', 'nonsmoking', 'pressure', 'crystal', 'exceptionally', 'towel', 'cleanliness', 'bug', 'well-maintained', 'lazy', 'maintained']\n",
      "﻿<value> ['ranges', 'half', 'accomodate', 'inclusive', 'deal', 'value', 'paying', 'worth', '!', 'extra', '$', 'pricy', 'overprice', '%', 'atmosphere', 'bargain', 'cost', 'charged', 'discount', 'fee', 'hotwire', 'price', 'vacation', 'tax', 'cash', 'quality', 'penny', 'low', 'cheap', 'bill', 'range', 'expectation', 'choice', 'usd', 'expensive', 'pesos', 'barter', 'priceline', 'honeymoon', 'paid', 'accomodation', 'rate', 'accommodate', 'money', 'pay', 'dollar', 'experience', 'resort', 'cheaper', 'definitely', 'anniversary', 'fraction', 'negotiate', 'dollars', 'star', 'quoted', 'rates', 'haggle', 'accommodation', 'prices', 'rating', 'taxes']\n",
      "<room> ['door', 'carpet', 'house', 'furniture', 'bathroom', 'ready', 'comfortable', 'decor', 'furnish', 'open', 'hear', 'renovation', 'light', 'rooms', 'screen', 'mansion', 'condition', 'bathtub', 'bed', 'sink', 'windows', 'kitchen', 'quiet', 'conditioner', 'sleep', 'conditioning', 'double', 'bedroom', 'shampoo', 'size', 'upgrade', 'huge', 'beds', 'floor', 'window', 'tub', 'shower', 'air', 'soap', 'view', 'suite', 'small', 'wall', 'large', 'room', 'pillow', 'television', 'housekeeping', 'queen', 'apartment', 'space', 'king', 'modern', 'mirror', 'twin', 'overlook', 'spacious', 'louver', 'toilet', 'bath', 'toiletry', 'balcony', 'decorate', 'stay', 'noise', 'chair', 'tower', 'tv', 'book', 'suit', 'facing', 'hairdryer', 'night', 'square', 'courtyard', 'channel', 'pillows', 'upgraded']\n",
      "1000 reviews done.\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "# Load Sentences_Aspects and Aspect keyword list\n",
    "import pickle\n",
    "\n",
    "# Use Keyword bootstrapped in own corpus\n",
    "#with open( 'Result-2000-7Processes.pickle', 'r' ) as resultfile:\n",
    "#    [ Sentence_aspects, T ] = pickle.load( resultfile )\n",
    "    \n",
    "# Check Aspect Keyword list\n",
    "for key in T.keys():\n",
    "    print key, T[ key ]\n",
    "    \n",
    "\n",
    "# Also extract ground truth aspect rating\n",
    "SampledTripAdvisor = []\n",
    "sentence_idx = 0\n",
    "for j, idx in enumerate( SampledIdx ):\n",
    "    temp = TripAdvisor[ idx ]\n",
    "    \n",
    "    sentences = []\n",
    "    sentence_aspects = []\n",
    "    for x in range( len( ReviewToSentences[ idx ] ) ):\n",
    "        sentences.append( Sentences[ sentence_idx ] )\n",
    "        sentence_aspects.append( Sentence_aspects[ sentence_idx ] )\n",
    "        sentence_idx += 1\n",
    "        \n",
    "    temp.sentences_final = sentences\n",
    "    temp.sentence_aspects = sentence_aspects\n",
    "    \n",
    "    if temp.aspects == 'NULL':\n",
    "        temp.aspect_rating = {}\n",
    "    else:\n",
    "        temp.aspect_rating = {}\n",
    "        aspectsList = temp.aspects.split('||||')\n",
    "        for item in aspectsList:\n",
    "            key, s_i = item.split('::')\n",
    "            temp.aspect_rating[ key ] = float( s_i )\n",
    "            \n",
    "    SampledTripAdvisor.append( temp )\n",
    "    if j>0 and j%1000 == 0:\n",
    "        print j, 'reviews done.'\n",
    "\n",
    "print len( SampledTripAdvisor )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Match between Review content and Sentence & Aspects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upon arrival we were greeted by Val at the front desk...he gave us an amazing room with a view of the Times Square New Year's Eve crystal ball. Thanks, Val...your great!! Our stay went great...location was perfect...room was large and very nice. We would definitely stay here again! Thank you Hilton for an amazing New Year's vacation\n",
      "[[u'great', u'year', u'eve', u'experience', u'arrival', u'greeted', u'val', u'front', u'desk', u'amazing', u'room', u'view', u'time', u'square', u'year', u'eve', u'crystal', u'ball'], [u'val', u'great'], [u'stay', u'great', u'location', u'perfect', u'room', u'large', u'nice'], [u'definitely', u'stay'], [u'hilton', u'amazing', u'year', u'vacation']]\n",
      "[[u'<room>'], [u'<location>'], [u'<location>', u'<room>'], [u'<location>', u'\\ufeff<value>', u'<room>'], [u'\\ufeff<value>']]\n",
      "{u'SleepQuality': 5.0, u'Service': 5.0, u'Cleanliness': 5.0, u'Value': 5.0, u'Location': 5.0, u'Rooms': 5.0}\n"
     ]
    }
   ],
   "source": [
    "print SampledTripAdvisor[1].content\n",
    "print SampledTripAdvisor[1].sentences_final\n",
    "print SampledTripAdvisor[1].sentence_aspects\n",
    "print SampledTripAdvisor[1].aspect_rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create $W_{d}$ for each review d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 reviews, transformed.\n",
      "1000 reviews, transformed.\n",
      "1500 reviews, transformed.\n",
      "2000 reviews, transformed.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "TripAdvisor = SampledTripAdvisor\n",
    "\n",
    "count = 0\n",
    "for idx in range( len( TripAdvisor ) ):\n",
    "    wd = []\n",
    "    for key in T.keys():\n",
    "        wdi = []\n",
    "        for word in Vocabulary:\n",
    "            word_frequency = 0\n",
    "            A_i_total_counts = 0\n",
    "            # word frequency of w_j in text of A_i\n",
    "            for sen_idx in range( len( TripAdvisor[ idx ].sentences_final ) ):\n",
    "                if key in TripAdvisor[ idx ].sentence_aspects[ sen_idx ]:\n",
    "                    word_frequency += TripAdvisor[ idx ].sentences_final[ sen_idx ].count( word )\n",
    "                    A_i_total_counts += len( TripAdvisor[ idx ].sentences_final[ sen_idx ] )\n",
    "            if A_i_total_counts == 0:\n",
    "                wdi.append( 0.0 )\n",
    "            else:\n",
    "                wdi.append( word_frequency / float( A_i_total_counts ) )\n",
    "        wd.append( wdi )\n",
    "    TripAdvisor[ idx ].wd = np.array( wd ) \n",
    "    count += 1\n",
    "    if count % 500 == 0:\n",
    "        print count, 'reviews, transformed.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E-step Updating\n",
    "$$L(d) = (\\hat{\\alpha}_d-\\mu)^\\intercal\\Sigma^{-1}(\\hat{\\alpha}_d-\\mu) +\n",
    "\\frac{(r_d - \\alpha_d^\\intercal S_d)^2}{\\delta^2} + \\gamma \\sum_{i=1}^k\n",
    "\\alpha_{di}(S_{di} - r_d )^2$$\n",
    "\n",
    "$$\\frac{\\partial L(d)}{\\partial \\alpha_{di}} =\n",
    "\\frac{2(\\alpha_d^\\intercal S_d - r_d)}{\\delta^2} \\frac{\\partial\n",
    "\\alpha_d^\\intercal S_d}{\\partial\n",
    "\\hat{\\alpha}_{di}} + \\gamma \\frac{\\partial \\sum_{j=1}^k \\alpha_{dj}(S_{dj} -\n",
    "r_d)^2 }{\\partial \\hat{\\alpha}_{di}} + \\frac{\\partial\n",
    "(\\hat{\\alpha}_d-\\mu)^\\intercal\\Sigma^{-1}(\\hat{\\alpha}_d-\\mu)}{\\partial\n",
    "\\hat{\\alpha}_{di}}$$\n",
    "\n",
    "$$\\frac{\\partial \\alpha_d^T S_d}{\\partial \\hat{\\alpha}_{di}} = \\alpha_{di}\n",
    "\\sum_{j=1}^k \\left[ \\tau(j=i)S_{dj}(1-\\alpha_{di}) - \\tau(j\\ne\n",
    "i)S_{dj}\\alpha_{dj} \\right]$$\n",
    "\n",
    "$$\\frac{\\partial \\sum_{j=1}^k \\alpha_{dj}(S_{dj} -\n",
    "r_d)^2}{\\partial \\hat{\\alpha}_{di}} = \\alpha_{di} \\sum_{j=1}^k \\left[\n",
    "\\tau(j=i)(S_{dj}-r_d)^2(1-\\alpha_{di}) - \\tau(j\\ne i)(S_{dj} - r_d)^2\\alpha_{dj}\n",
    "\\right]$$\n",
    "\n",
    "$$\\frac{\\partial\n",
    "(\\hat{\\alpha}_d-\\mu)^\\intercal\\Sigma^{-1}(\\hat{\\alpha}_d-\\mu)}{\\partial\n",
    "\\hat{\\alpha}_{di}} = 2 (\\hat{\\alpha}_d - \\mu) \\Sigma^{-1} \\cdot I \\cdot\n",
    "\\frac{\\partial \\hat{\\alpha}_d}{\\partial \\hat{\\alpha}_{di}} =\n",
    "2\\sum_{j=1}^k \\Sigma_{ji}^{-1}(\\hat{\\alpha}_{dj} - \\mu_j)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from numpy.linalg import inv\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "gamma = 0.5\n",
    "\n",
    "# Infer S_d for each review\n",
    "def Estep_sd( review, beta ):\n",
    "    aspectLength = beta.shape[0]\n",
    "    aspect_rating = []\n",
    "    for rowid in range( aspectLength ):\n",
    "        s_di = review.wd[ rowid ].dot( beta[ rowid ].T )\n",
    "        aspect_rating.append( s_di )\n",
    "    return np.array( [ aspect_rating ] ).T\n",
    "\n",
    "# Function L( alphad_hat )\n",
    "def L_alphad_hat( alphad_hat, mu, Sigma_inv, rating, delta_square, Sd, gamma ):\n",
    "    term1 = ( alphad_hat - mu ).T.dot( Sigma_inv ).dot( alphad_hat - mu )[0,0]\n",
    "    \n",
    "    expsum = np.sum( np.exp( alphad_hat ) )\n",
    "    alphad = np.exp( alphad_hat ) / expsum\n",
    "    term2 = ( rating - alphad.T.dot( Sd ) )** 2 / delta_square\n",
    "    \n",
    "    term3 = gamma * alphad.dot( ( Sd - rating ) ** 2 )\n",
    "    return term1 + term2 + term3\n",
    "\n",
    "# Derivative of function L( alphad_hat )\n",
    "def dLdaphad_hat( alphad_hat, mu, Sigma_inv, rating, delta_square, Sd, gamma ):\n",
    "    expsum = np.sum( np.exp( alphad_hat ) )\n",
    "    alphad = np.exp( alphad_hat ) / expsum\n",
    "    \n",
    "    aspectLength = mu.shape[0]\n",
    "    \n",
    "    derivative1 = np.zeros( ( aspectLength , aspectLength ) )\n",
    "    derivative2 = np.zeros( ( aspectLength , aspectLength ) )\n",
    "    for i in range( aspectLength ):\n",
    "        indicator = np.zeros( ( aspectLength ,1 ) )\n",
    "        indicator[ i, 0 ] = 1.0\n",
    "        derivative1[ i, i ] = Sd.T.dot( -alphad + indicator )[0,0]\n",
    "        derivative2[ i, i ] = ( ( Sd - rating ) ** 2 ).T.dot( -alphad + indicator )[0,0]\n",
    "    \n",
    "    term1 = 2 * ( alphad.T.dot( Sd ) - rating ) / delta_square * ( derivative1.dot( alphad ) )\n",
    "    \n",
    "    term2 = gamma * ( derivative2.dot( alphad ) )\n",
    "    \n",
    "    term3 = 2 * ( alphad_hat - mu ).T.dot( Sigma_inv ).T\n",
    "    return term1 + term2 + term3\n",
    "\n",
    "# Infer alphad based on LBFGS algorithm\n",
    "def Estep_alphad( alphad_hat0, review, mu, Sigma, delta_square, gamma ):\n",
    "    \n",
    "    Sigma_inv = inv( Sigma )\n",
    "    \n",
    "    print 'Begin \\hat{alpha}_d inference.'\n",
    "    res = minimize( L_alphad_hat, alphad_hat0, \n",
    "                   args=( mu, Sigma_inv, review.rating, delta_square, review.Sd, gamma ),\n",
    "                   method='bfgs', jac=dLdaphad_hat, tol= 1e-2, options={'maxiter':500,'disp':True} )\n",
    "    print '\\hat{alpha}_d inference done.'\n",
    "    \n",
    "    return res.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M-step Updating\n",
    "$$\n",
    "\\mu_{(t+1)} = \\underset{\\mu}{\\arg\\min} \\sum_{ d\\in D} (\\hat{\\alpha}_d -\n",
    "\\mu)\\Sigma^{-1}(\\hat{\\alpha}_d - \\mu) = \\frac{1}{|D|} \\sum_{d \\in D}\n",
    "\\hat{\\alpha}_d\n",
    "$$\n",
    "$$\n",
    "\\begin{split}\n",
    "\\Sigma_{(t+1)} = & \\underset{\\Sigma}{\\arg\\min} \\sum_{ d\\in D}\n",
    "\\left[ (\\hat{\\alpha}_d - \\mu)\\Sigma^{-1}(\\hat{\\alpha}_d - \\mu) + \\log |\\Sigma|\n",
    "\\right ] \\\\\n",
    "= & \\frac{1}{|D|} \\sum_{d \\in D} (\\hat{\\alpha}_d -\n",
    "\\mu_{(t+1)})^\\intercal (\\hat{\\alpha}_d - \\mu_{(t+1)})\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "\\begin{equation*}\n",
    "L(D) = \\sum_{d \\in D} \\left[ \\log\\delta^2 + \\frac{(r_d -\n",
    "\\alpha_d^TS_d)^2}{\\delta^2} + \\gamma \\sum_{i=1}^k \\alpha_{di}(S_{di} - r_d )^2 \\right] + \\lambda \\beta^\\intercal\n",
    "\\beta\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "\\frac{\\partial L(\\beta)}{\\partial \\beta_i} = & \\sum_{d \\in D} \\left[\n",
    "\\frac{ 2( \\alpha_d^\\intercal S_d - r_d ) }{\\delta^2} \\frac{\\partial\n",
    "\\alpha_d^\\intercal S_d}{\\partial \\beta_i} + 2 \\gamma \\alpha_{di} (S_{di} -\n",
    "r_d) \\frac{\\partial S_{di}}{\\partial \\beta_i} \\right] + 2\\lambda\\beta_i \\\\\n",
    "= & 2 \\sum_{d \\in D} \\alpha_{di} \\left[\n",
    "\\frac{ ( \\alpha_d^\\intercal S_d - r_d ) }{\\delta^2} + \\gamma (S_{di}\n",
    "- r_d) \\right] \\frac{\\partial S_{di}}{\\partial \\beta_i} + 2\\lambda\\beta_i\n",
    "\\end{split}\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "\\delta_{(t+1)}^2 = \\underset{\\delta}{\\arg\\min} \\sum_{d \\in D} \\left[ \\log\n",
    "\\delta^2 + \\frac{(r_d - \\alpha_d^\\intercal S_d)^2}{\\delta^2} \\right] =\n",
    "\\frac{1}{|D|} \\sum_{d \\in D} (r_d - \\alpha_d^\\intercal S_d)^2\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Update mu\n",
    "def Mstep_mu(  TripAdvisor ):\n",
    "    mu = np.zeros_like( TripAdvisor[0].alphad )\n",
    "    for review in TripAdvisor:\n",
    "        mu += review.alphad_hat\n",
    "    mu /= len( TripAdvisor )\n",
    "    return mu\n",
    "\n",
    "# Update Sigma\n",
    "def Mstep_Sigma( TripAdvisor, mu ):\n",
    "    aspectLength = TripAdvisor[0].alphad_hat.shape[0]\n",
    "    Sigma = np.zeros( ( aspectLength, aspectLength ) )\n",
    "    for review in TripAdvisor:\n",
    "        Sigma += ( review.alphad_hat - mu ).T.dot( review.alphad_hat - mu )\n",
    "    Sigma /= len( TripAdvisor )\n",
    "    return Sigma\n",
    "\n",
    "def Mstep_delta_square( TripAdvisor ):\n",
    "    aspectLength = TripAdvisor[0].alphad_hat.shape[0]\n",
    "    delta_square = np.zeros( ( aspectLength, aspectLength ) )\n",
    "    for review in TripAdvisor:\n",
    "        delta_square += ( review.rating - review.alphad.T.dot( review.Sd ) ) ** 2\n",
    "    delta_square /= len( TripAdvisor )\n",
    "    return delta_square\n",
    "\n",
    "def L_beta( beta, TripAdvisor, delta_square, gamma, reg_lambda ):\n",
    "    L = 0\n",
    "    for review in TripAdvisor:\n",
    "        L += ( review.rating - review.alphad.T.dot( review.Sd ) ) ** 2 / delta_square + gamma * ( review.alphad.T.dot( ( review.Sd - review.rating ) ** 2 ) )\n",
    "    L += reg_lambda * np.trace( beta.T.dot( beta ) )\n",
    "    return\n",
    "\n",
    "def dLdbeta( beta, TripAdvisor, delta_square, gamma, reg_lambda ):\n",
    "    Rows, Cols = TripAdvisor[0].wd.shape\n",
    "    dldbeta = np.zeros( ( Rows, Cols ) )\n",
    "    for review in TripAdvisor:\n",
    "        M_alphad = np.diag( review.alphad.T[0] )\n",
    "        dldbeta += np.diag( ( M_alphad.dot( ( review.alphad.T.dot( review.Sd ) - review.rating ) / delta_square\n",
    "                                + gamma * ( review.Sd - review.rating ) ) ).T[0] ).dot( np.diag( review.Sd.T[0] ).dot( review.wd ) )\n",
    "    dldbeta += reg_lambda * beta\n",
    "    return dldbeta\n",
    "\n",
    "def Mstep_beta( beta0, TripAdvisor, delta_square, gamma, reg_lambda ):\n",
    "    \n",
    "    print 'Begin beta inference.'\n",
    "    res = minimize( L_beta, beta0, args=( TripAdvisor, delta_square, gamma, reg_lambda ),\n",
    "                   method='bfgs', jac=dLdbeta, tol= 1e-2, options={'maxiter':5000,'disp':True} )\n",
    "    print 'beta inference done.'\n",
    "    \n",
    "    return res.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Likelihood estimator\n",
    "1. For each review d infer $s_d$ and $\\alpha_d$ with current $\\Theta_t$\n",
    "2. Based on $r_d$ and $\\alpha_d$ maximization, update $\\Theta_{(t+1)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin \\hat{alpha}_d inference.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-bfad8244569b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[0malphad_hat0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;33m(\u001b[0m \u001b[0maspectLength\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         TripAdvisor[ idx ].alphad_hat = Estep_alphad( alphad_hat0, TripAdvisor[ idx ],\n\u001b[1;32m---> 50\u001b[1;33m                                                      mu, Sigma, delta_square, gamma )\n\u001b[0m\u001b[0;32m     51\u001b[0m         \u001b[0mexpSum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mTripAdvisor\u001b[0m\u001b[1;33m[\u001b[0m \u001b[0midx\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malphad_hat\u001b[0m \u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[0mTripAdvisor\u001b[0m\u001b[1;33m[\u001b[0m \u001b[0midx\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malphad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mTripAdvisor\u001b[0m\u001b[1;33m[\u001b[0m \u001b[0midx\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malphad_hat\u001b[0m \u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mexpSum\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-34-6c7d330f4593>\u001b[0m in \u001b[0;36mEstep_alphad\u001b[1;34m(alphad_hat0, review, mu, Sigma, delta_square, gamma)\u001b[0m\n\u001b[0;32m     56\u001b[0m     res = minimize( L_alphad_hat, alphad_hat0, \n\u001b[0;32m     57\u001b[0m                    \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSigma_inv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreview\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrating\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelta_square\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreview\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m                    method='bfgs', jac=dLdaphad_hat, tol= 1e-2, options={'maxiter':500,'disp':True} )\n\u001b[0m\u001b[0;32m     59\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[1;34m'\\hat{alpha}_d inference done.'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/scipy/optimize/_minimize.pyc\u001b[0m in \u001b[0;36mminimize\u001b[1;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[0;32m    439\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_minimize_cg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjac\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    440\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'bfgs'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 441\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_minimize_bfgs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjac\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    442\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'newton-cg'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    443\u001b[0m         return _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/scipy/optimize/optimize.pyc\u001b[0m in \u001b[0;36m_minimize_bfgs\u001b[1;34m(fun, x0, args, jac, callback, gtol, norm, eps, maxiter, disp, return_all, **unknown_options)\u001b[0m\n\u001b[0;32m    863\u001b[0m             \u001b[0malpha_k\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mold_fval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mold_old_fval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgfkp1\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    864\u001b[0m                      _line_search_wolfe12(f, myfprime, xk, pk, gfk,\n\u001b[1;32m--> 865\u001b[1;33m                                           old_fval, old_old_fval)\n\u001b[0m\u001b[0;32m    866\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0m_LineSearchError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    867\u001b[0m             \u001b[1;31m# Line search failed to find a better solution.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/scipy/optimize/optimize.pyc\u001b[0m in \u001b[0;36m_line_search_wolfe12\u001b[1;34m(f, fprime, xk, pk, gfk, old_fval, old_old_fval, **kwargs)\u001b[0m\n\u001b[0;32m    704\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ignore'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLineSearchWarning\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    705\u001b[0m             ret = line_search_wolfe2(f, fprime, xk, pk, gfk,\n\u001b[1;32m--> 706\u001b[1;33m                                      old_fval, old_old_fval)\n\u001b[0m\u001b[0;32m    707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    708\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/scipy/optimize/linesearch.pyc\u001b[0m in \u001b[0;36mline_search_wolfe2\u001b[1;34m(f, myfprime, xk, pk, gfk, old_fval, old_old_fval, args, c1, c2, amax)\u001b[0m\n\u001b[0;32m    280\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m     alpha_star, phi_star, old_fval, derphi_star = scalar_search_wolfe2(\n\u001b[1;32m--> 282\u001b[1;33m             phi, derphi, old_fval, old_old_fval, derphi0, c1, c2, amax)\n\u001b[0m\u001b[0;32m    283\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    284\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mderphi_star\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/scipy/optimize/linesearch.pyc\u001b[0m in \u001b[0;36mscalar_search_wolfe2\u001b[1;34m(phi, derphi, phi0, old_phi0, derphi0, c1, c2, amax)\u001b[0m\n\u001b[0;32m    377\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0malpha1\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    378\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 379\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mphi_a1\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mphi0\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mc1\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0malpha1\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mderphi0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    380\u001b[0m            \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mphi_a1\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mphi_a0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m             \u001b[0malpha_star\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mphi_star\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mderphi_star\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "from numpy.random import uniform\n",
    "\n",
    "# Calculate likelihood value of the whole corpus\n",
    "def L_D( TripAdvisor, Sigma, mu, delta_square, gamma, beta, reg_lambda ):\n",
    "    L = 0\n",
    "    for review in TripAdvisor:     \n",
    "        term1 = np.log( np.linalg.det( Sigma ) ) \n",
    "        term2 = ( review.alphad_hat - mu ).T.dot( inv( Sigma ) ).dot( review.alphad_hat - mu )\n",
    "        term3 = np.log( delta_square ) \n",
    "        term4 = ( review.rating - review.alphad.T.dot( review.Sd ) ) / delta_square \n",
    "        term5 = gamma * review.alphad.T.dot( ( review.Sd - review.rating )** 2 ) \n",
    "        L += ( term1 + term2 + term3 + term4 + term5 )\n",
    "    L += reg_lambda * np.sum( beta ** 2 )\n",
    "    return L\n",
    "\n",
    "# Initialize iteration parameters\n",
    "iter_num = 0\n",
    "max_iter = 10\n",
    "convergence = 1e-4\n",
    "diff = 10\n",
    "\n",
    "# Initialize corpus parameters\n",
    "aspectLength = len( T.keys() )\n",
    "vocaLength = len( Vocabulary )\n",
    "\n",
    "Sigma = np.identity( aspectLength )\n",
    "mu = 2 * uniform( size= aspectLength ).reshape( aspectLength, 1 ) - 1.0\n",
    "\n",
    "delta_square = 1.0\n",
    "gamma = 0.5\n",
    "\n",
    "beta = []\n",
    "for i in range( aspectLength ):\n",
    "    beta.append( np.random.sample( vocaLength ) )\n",
    "beta = np.array( beta )\n",
    "\n",
    "reg_lambda = 2.0\n",
    "L0 = 1.0\n",
    "\n",
    "\n",
    "while ( diff > convergence and iter_num < max_iter ) or iter_num < min( 8, max_iter ):\n",
    "    \n",
    "    # E-step\n",
    "    for idx in range( len( TripAdvisor ) ):\n",
    "        TripAdvisor[ idx ].Sd = Estep_sd( TripAdvisor[ idx ] , beta )\n",
    "        alphad_hat0 = np.zeros( ( aspectLength, 1 ) )\n",
    "        TripAdvisor[ idx ].alphad_hat = Estep_alphad( alphad_hat0, TripAdvisor[ idx ],\n",
    "                                                     mu, Sigma, delta_square, gamma )\n",
    "        expSum = np.sum( np.exp( TripAdvisor[ idx ].alphad_hat ) )\n",
    "        TripAdvisor[ idx ].alphad = np.exp( TripAdvisor[ idx ].alphad_hat ) / expSum\n",
    "    \n",
    "    # M-step\n",
    "    mu = Mstep_mu(  TripAdvisor )\n",
    "    \n",
    "    # Avoid update Sigma too often\n",
    "    if iter_num % 4 == 3:\n",
    "        Sigma = Mstep_Sigma( TripAdvisor, mu )\n",
    "        \n",
    "    delta_square = Mstep_delta_square( TripAdvisor )\n",
    "    beta = Mstep_beta( beta, TripAdvisor, delta_square, gamma, reg_lambda )\n",
    "    \n",
    "    L1 = L_D( TripAdvisor, Sigma, mu, delta_square, gamma, beta, reg_lambda )\n",
    "    \n",
    "    diff = ( L1 - L0 ) / float( L0 )\n",
    "    \n",
    "    L0 = L1\n",
    "    \n",
    "    iter_num += 1\n",
    "    print 'Iteration:', iter_num, 'done.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('Final_Results', 'w' ) as resultFile:\n",
    "    pickle.dump( [ TripAdvisor, Sigma, mu, delta_square, gamma, beta, reg_lambda ] , resultFile )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "Wang, H., Lu, Y., and Zhai, C. (2010). Latent aspect rating analysis on review text data: a rating regression approach. In Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 783–792. ACM."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
