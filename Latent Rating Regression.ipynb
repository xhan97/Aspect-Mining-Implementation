{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Define Review Class\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk.data\n",
    "\n",
    "with open( 'stopwords.dat', 'r' ) as stopfile:\n",
    "    en_stop = stopfile.readlines()\n",
    "    en_stop = [ item.split('\\n')[0] for item in en_stop if not item in ['\\n'] ]\n",
    "\n",
    "class Review:\n",
    "    # Initialization\n",
    "    def __init__(self, hotel='', title='', rating='', aspects='', content='' ):\n",
    "        self.hotel = hotel\n",
    "        self.title = title\n",
    "        self.rating = rating\n",
    "        self.aspects = aspects\n",
    "        self.content = content\n",
    "        \n",
    "    def Sentences( self ):\n",
    "        tokenizer = nltk.data.load( 'tokenizers/punkt/english.pickle' )\n",
    "        self.sentences = tokenizer.tokenize( self.title.lower() + self.content.lower() )\n",
    "        \n",
    "    def pre_process( self ):\n",
    "        tokenizer = RegexpTokenizer( r'\\w+' )\n",
    "        temp = []\n",
    "        for sentence in self.sentences:\n",
    "            tokens = tokenizer.tokenize( sentence )\n",
    "            tokens_nostop = [ item for item in tokens if not unicode( item ) in en_stop ]\n",
    "            temp.append( tokens_nostop )\n",
    "        self.tokens = temp\n",
    "        \n",
    "    def removeTerms( self, Vocabulary ):\n",
    "        terms_removed = []\n",
    "        for sentence in self.tokens:\n",
    "            new_tokens = [ item for item in sentence if  item in Vocabulary ]\n",
    "            terms_removed.append( new_tokens )\n",
    "        self.terms_removed = terms_removed\n",
    "        \n",
    "    def Stemmer( self ):\n",
    "        p_stemmer = PorterStemmer()\n",
    "        sentences_final = []\n",
    "        for sentence in self.terms_removed:\n",
    "            sentence = [ p_stemmer.stem( item ) for item in sentence ]\n",
    "            sentences_final.append( sentence )\n",
    "        self.sentences_final = sentences_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tripadvisor loaded.\n"
     ]
    }
   ],
   "source": [
    "## Load TripAdvisor Reviews which includes Overall Rating, Aspect Rating and review contents\n",
    "\n",
    "import csv\n",
    "\n",
    "TripAdvisor = []\n",
    "with open( 'tripadvisor_reviews.csv' , 'r' ) as csvfile:\n",
    "    Reviewreader = csv.reader( csvfile, delimiter=',' )\n",
    "    for row in Reviewreader:\n",
    "        hotel = row[1].decode('utf-8')\n",
    "        title = row[2].decode('utf-8')\n",
    "        rating = float( row[5] ) \n",
    "        aspects = row[6].decode('utf-8')\n",
    "        content = row[7].decode('utf-8')\n",
    "        TripAdvisor.append( Review( hotel, title, rating, aspects, content ) )\n",
    "    print 'Tripadvisor loaded.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 reviews have been processed.\n",
      "20000 reviews have been processed.\n",
      "30000 reviews have been processed.\n",
      "40000 reviews have been processed.\n",
      "50000 reviews have been processed.\n",
      "60000 reviews have been processed.\n",
      "70000 reviews have been processed.\n",
      "80000 reviews have been processed.\n",
      "90000 reviews have been processed.\n",
      "100000 reviews have been processed.\n",
      "110000 reviews have been processed.\n",
      "120000 reviews have been processed.\n",
      "130000 reviews have been processed.\n",
      "140000 reviews have been processed.\n",
      "150000 reviews have been processed.\n",
      "160000 reviews have been processed.\n",
      "170000 reviews have been processed.\n",
      "180000 reviews have been processed.\n",
      "190000 reviews have been processed.\n",
      "200000 reviews have been processed.\n",
      "210000 reviews have been processed.\n",
      "220000 reviews have been processed.\n",
      "230000 reviews have been processed.\n",
      "240000 reviews have been processed.\n",
      "250000 reviews have been processed.\n",
      "260000 reviews have been processed.\n",
      "270000 reviews have been processed.\n",
      "280000 reviews have been processed.\n",
      "290000 reviews have been processed.\n",
      "300000 reviews have been processed.\n",
      "310000 reviews have been processed.\n",
      "320000 reviews have been processed.\n",
      "330000 reviews have been processed.\n",
      "340000 reviews have been processed.\n",
      "350000 reviews have been processed.\n",
      "360000 reviews have been processed.\n",
      "370000 reviews have been processed.\n",
      "380000 reviews have been processed.\n",
      "390000 reviews have been processed.\n",
      "400000 reviews have been processed.\n",
      "410000 reviews have been processed.\n",
      "420000 reviews have been processed.\n",
      "430000 reviews have been processed.\n"
     ]
    }
   ],
   "source": [
    "# Get Preprocessing for all the reviews\n",
    "i = 0\n",
    "for idx in range( len( TripAdvisor ) ):\n",
    "    TripAdvisor[ idx ].Sentences()\n",
    "    TripAdvisor[ idx ].pre_process()\n",
    "    i += 1\n",
    "    if i%10000 == 0:\n",
    "        print i, 'reviews have been processed.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Count the frequency of each token\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "Tokens = []\n",
    "for review in TripAdvisor:\n",
    "    for tokens in review.tokens:\n",
    "        Tokens += tokens\n",
    "        \n",
    "TermList = Counter( Tokens )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get terms occuring less than 5 in the corpus\n",
    "RemoveTermList = []\n",
    "for token in Tokens:\n",
    "    if TermList[ token ] < 5:\n",
    "        RemoveTermList.append( token )\n",
    "\n",
    "RemoveTermList = list( set( RemoveTermList ) )\n",
    "Tokens = list( set( Tokens ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158722 118896\n",
      "39826\n"
     ]
    }
   ],
   "source": [
    "# Build the vocabulary for the corpus\n",
    "Vocabulary = []\n",
    "i = 0\n",
    "print len( Tokens ), len( RemoveTermList )\n",
    "\n",
    "Vocabulary =  list( set( Tokens ).difference( set( RemoveTermList ) ) )\n",
    "print len( Vocabulary )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "220000\n",
      "230000\n",
      "240000\n",
      "250000\n",
      "260000\n",
      "270000\n",
      "280000\n",
      "290000\n",
      "300000\n",
      "310000\n",
      "320000\n",
      "330000\n",
      "340000\n",
      "350000\n",
      "360000\n",
      "370000\n",
      "380000\n",
      "390000\n",
      "400000\n",
      "410000\n",
      "420000\n",
      "430000\n"
     ]
    }
   ],
   "source": [
    "# Remove terms appearing less than 5 times\n",
    "i=0\n",
    "for idx in range( len( TripAdvisor ) ):\n",
    "    TripAdvisor[ idx ].removeTerms( Vocabulary )\n",
    "    i += 1\n",
    "    if i%10000 == 0:\n",
    "        print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "220000\n",
      "230000\n",
      "240000\n",
      "250000\n",
      "260000\n",
      "270000\n",
      "280000\n",
      "290000\n",
      "300000\n",
      "310000\n",
      "320000\n",
      "330000\n",
      "340000\n",
      "350000\n",
      "360000\n",
      "370000\n",
      "380000\n",
      "390000\n",
      "400000\n",
      "410000\n",
      "420000\n",
      "430000\n"
     ]
    }
   ],
   "source": [
    "# Stemming each word to its root with Porter Stemmer\n",
    "i = 0\n",
    "for idx in range( len( TripAdvisor ) ):\n",
    "    TripAdvisor[ idx ].Stemmer()\n",
    "    i += 1\n",
    "    if i%10000 == 0:\n",
    "        print i\n",
    "        \n",
    "# Preprocessing done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save Variables into file\n",
    "import pickle\n",
    "\n",
    "with open( 'TripAdvisorReviews.pickle', 'w' ) as TripAdvisorfile:\n",
    "    pickle.dump( [ TripAdvisor ] , TripAdvisorfile )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aspect Segmentation -- Bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<value> ['value', 'price', 'quality', 'worth']\n",
      "<room> ['room', 'suite', 'view', 'bed']\n",
      "<location> ['location', 'traffic', 'minute', 'restaurant']\n",
      "<cleanliness> ['clean', 'dirty', 'maintain', 'smell']\n",
      "<front desk> ['stuff', 'check', 'help', 'reservation']\n",
      "<service> ['service', 'food', 'breakfast', 'buffet']\n",
      "<business service> ['business', 'center', 'computer', 'internet']\n"
     ]
    }
   ],
   "source": [
    "# Load seed Topic List\n",
    "T = {}\n",
    "with open( 'hotel_bootstrapping.dat', 'r' ) as seedfile:\n",
    "    Reader = csv.reader( seedfile, delimiter='\\t' )\n",
    "    for row in Reader:\n",
    "        T[ row[0].decode('utf-8') ] = row[1:]\n",
    "        print row[0], row[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p = 5  # selection threshold\n",
    "max_iter = 10 # iteration step limit\n",
    "\n",
    "\n",
    "iter_num = 0 # Initialize iter_num and keyword list changed.\n",
    "Flag = True\n",
    "\n",
    "while iter_num < max_iter and Flag:\n",
    "    Flag = False # set keyword list to unchanged\n",
    "    \n",
    "    # Match the aspect keywords in each sentence and record its matching\n",
    "    for idx in range( len( TripAdvisor ) ):\n",
    "        aspectCounts = [] # count(i) for each aspect\n",
    "        sentence_aspects = []\n",
    "        for sentence in TripAdvisor[ idx ].sentences_final:\n",
    "            aspectcount = {}\n",
    "            aspects = [] # aspects of this sentence\n",
    "            countMax = 0 # maximum count(i)\n",
    "            for key in T.keys():\n",
    "                count = 0\n",
    "                for keyword in T[ key ]:\n",
    "                    count += sentence.count( keyword )\n",
    "                aspectcount[ key ] = count\n",
    "                if count > countMax:\n",
    "                    countMax = count\n",
    "            AspectCounts.append( aspectcount )\n",
    "        \n",
    "            for key in T.keys():\n",
    "                if aspectcount[ key ] == countMax:\n",
    "                    aspects.append( key )\n",
    "            \n",
    "            sentence_aspects.append( aspects )\n",
    "                   \n",
    "        TripAdvisor[ idx ].aspectCounts = aspectCounts\n",
    "        TripAdvisor[ idx ].sentence_aspects = sentence_aspects\n",
    "    # sentence annotation with aspect assignment done\n",
    "    \n",
    "    # Calculate chi^2 value for each word in Vocabulary under every aspect\n",
    "    for key in T.keys():\n",
    "        chi_list = []\n",
    "        for word in Vocabulary:\n",
    "            C_1, C_2, C_3, C_4, C = 0, 0, 0, 0, 0\n",
    "            for review in TripAdvisor:\n",
    "                for idx in range( len( review.sentences_final ) ):\n",
    "                    C += review.sentences_final[idx].count( word )\n",
    "                    if key in review.sentence_aspects[ idx ]:\n",
    "                        C_1 += review.sentences_final[idx].count( word )\n",
    "                        if not word in review.sentences_final[ idx ]:\n",
    "                            C_3 += 1\n",
    "                    else:\n",
    "                        C_2 += review.sentences_final[idx].count( word )\n",
    "                        if not word in review.sentences_final[ idx ]:\n",
    "                            C_4 += 1\n",
    "            denominator = (C_1 + C_3) * (C_2 + C_4) * (C_1 + C_2) * (C_3 + C_4)\n",
    "            nominator = C * ( C_1*C_4 - C_2*C_3 ) * ( C_1*C_4 - C_2*C_3 )\n",
    "            if denominator > 0:\n",
    "                chi = nominator / float( denominator )\n",
    "            else:\n",
    "                chi = 0.0\n",
    "            chi_list.append( ( word, chi ) )\n",
    "        Chi_sorted = sorted( chi_list , key=lambda tup: tup[1] , reverse = True )\n",
    "    \n",
    "        # Joint top p words into aspect keywords list\n",
    "        len_original = len( T[ key ] )\n",
    "        for idx in range( p ):\n",
    "            T[ key ].append( Chi_sorted[ idx ][0] )\n",
    "        T[ key ] = list( set( T[ key ] ) )\n",
    "    \n",
    "        # Test whether the aspect keyword list has changed.\n",
    "        if len( T[ key ] ) > len_original:\n",
    "            Flag = True\n",
    "    iter_num += 1\n",
    "    print iter_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create $W_{d}$ for each review d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "for idx in range( len( TripAdvisor ) ):\n",
    "    TripAdvisor[ idx ]."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
