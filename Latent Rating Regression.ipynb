{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Process of Latent Aspect Rating Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Define Review Class\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk.data\n",
    "\n",
    "with open( 'stopwords.dat', 'r' ) as stopfile:\n",
    "    en_stop = stopfile.readlines()\n",
    "    en_stop = [ item.split('\\n')[0] for item in en_stop if not item in ['\\n'] ]\n",
    "\n",
    "class Review:\n",
    "    # Initialization\n",
    "    def __init__(self, hotel='', title='', rating='', aspects='', content='' ):\n",
    "        self.hotel = hotel\n",
    "        self.title = title\n",
    "        self.rating = rating\n",
    "        self.aspects = aspects\n",
    "        self.content = content\n",
    "        \n",
    "    def Sentences( self ):\n",
    "        tokenizer = nltk.data.load( 'tokenizers/punkt/english.pickle' )\n",
    "        self.sentences = tokenizer.tokenize( self.title.lower() + self.content.lower() )\n",
    "        \n",
    "    def pre_process( self ):\n",
    "        tokenizer = RegexpTokenizer( r'\\w+' )\n",
    "        temp = []\n",
    "        for sentence in self.sentences:\n",
    "            tokens = tokenizer.tokenize( sentence )\n",
    "            tokens_nostop = [ item for item in tokens if not unicode( item ) in en_stop ]\n",
    "            temp.append( tokens_nostop )\n",
    "        self.tokens = temp\n",
    "        \n",
    "    def removeTerms( self, Vocabulary ):\n",
    "        terms_removed = []\n",
    "        for sentence in self.tokens:\n",
    "            new_tokens = [ item for item in sentence if  item in Vocabulary ]\n",
    "            terms_removed.append( new_tokens )\n",
    "        self.terms_removed = terms_removed\n",
    "        \n",
    "    def Stemmer( self ):\n",
    "        p_stemmer = PorterStemmer()\n",
    "        sentences_final = []\n",
    "        for sentence in self.terms_removed:\n",
    "            sentence = [ p_stemmer.stem( item ) for item in sentence ]\n",
    "            sentences_final.append( sentence )\n",
    "        self.sentences_final = sentences_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tripadvisor loaded.\n"
     ]
    }
   ],
   "source": [
    "## Load TripAdvisor Reviews which includes Overall Rating, Aspect Rating and review contents\n",
    "\n",
    "import csv\n",
    "\n",
    "TripAdvisor = []\n",
    "with open( 'tripadvisor_reviews.csv' , 'r' ) as csvfile:\n",
    "    Reviewreader = csv.reader( csvfile, delimiter=',' )\n",
    "    for row in Reviewreader:\n",
    "        hotel = row[1].decode('utf-8')\n",
    "        title = row[2].decode('utf-8')\n",
    "        rating = float( row[5] ) \n",
    "        aspects = row[6].decode('utf-8')\n",
    "        content = row[7].decode('utf-8')\n",
    "        TripAdvisor.append( Review( hotel, title, rating, aspects, content ) )\n",
    "    print 'Tripadvisor loaded.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get Preprocessing for all the reviews\n",
    "i = 0\n",
    "for idx in range( len( TripAdvisor ) ):\n",
    "    TripAdvisor[ idx ].Sentences()\n",
    "    TripAdvisor[ idx ].pre_process()\n",
    "    i += 1\n",
    "    if i%10000 == 0:\n",
    "        print i, 'reviews have been processed.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Count the frequency of each token\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "Tokens = []\n",
    "for review in TripAdvisor:\n",
    "    for tokens in review.tokens:\n",
    "        Tokens += tokens\n",
    "        \n",
    "TermList = Counter( Tokens )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get terms occuring less than 5 in the corpus\n",
    "RemoveTermList = []\n",
    "for token in Tokens:\n",
    "    if TermList[ token ] < 5:\n",
    "        RemoveTermList.append( token )\n",
    "\n",
    "RemoveTermList = list( set( RemoveTermList ) )\n",
    "Tokens = list( set( Tokens ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158722 118896\n",
      "39826\n"
     ]
    }
   ],
   "source": [
    "# Build the vocabulary for the corpus\n",
    "Vocabulary = []\n",
    "i = 0\n",
    "print len( Tokens ), len( RemoveTermList )\n",
    "\n",
    "Vocabulary =  list( set( Tokens ).difference( set( RemoveTermList ) ) )\n",
    "print len( Vocabulary )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove terms appearing less than 5 times\n",
    "i=0\n",
    "for idx in range( len( TripAdvisor ) ):\n",
    "    TripAdvisor[ idx ].removeTerms( Vocabulary )\n",
    "    i += 1\n",
    "    if i%10000 == 0:\n",
    "        print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Stemming each word to its root with Porter Stemmer\n",
    "i = 0\n",
    "for idx in range( len( TripAdvisor ) ):\n",
    "    TripAdvisor[ idx ].Stemmer()\n",
    "    i += 1\n",
    "    if i%10000 == 0:\n",
    "        print i\n",
    "        \n",
    "# Preprocessing done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save Variables into file\n",
    "import pickle\n",
    "\n",
    "with open( 'TripAdvisorReviews.pickle', 'w' ) as TripAdvisorfile:\n",
    "    pickle.dump( [ TripAdvisor ] , TripAdvisorfile )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open( 'Vocabulary.pickle', 'w' ) as Vfile:\n",
    "    pickle.dump( [Vocabulary], Vfile )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load saved processed reviews and vocabulary\n",
    "import pickle\n",
    "\n",
    "with open( 'TripAdvisorReviews.pickle', 'r' ) as TripAdvisorfile:\n",
    "    [ TripAdvisor ] = pickle.load( TripAdvisorfile )\n",
    "    \n",
    "with open( 'Vocabulary.pickle', 'r' ) as Vfile:\n",
    "    [ Vocabulary ] = pickle.load( Vfile )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TripSentences_final = [ review.sentences_final for review in TripAdvisor ]\n",
    "\n",
    "with open( 'review_sentences.pickle', 'w') as sentence_file:\n",
    "    pickle.dump( [ TripSentences_final ], sentence_file )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aspect Segmentation -- Bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<value> ['value', 'price', 'quality', 'worth']\n",
      "<room> ['room', 'suite', 'view', 'bed']\n",
      "<location> ['location', 'traffic', 'minute', 'restaurant']\n",
      "<cleanliness> ['clean', 'dirty', 'maintain', 'smell']\n",
      "<front desk> ['stuff', 'check', 'help', 'reservation']\n",
      "<service> ['service', 'food', 'breakfast', 'buffet']\n",
      "<business service> ['business', 'center', 'computer', 'internet']\n"
     ]
    }
   ],
   "source": [
    "# Load seed Topic List\n",
    "import csv\n",
    "\n",
    "T = {}\n",
    "with open( 'hotel_bootstrapping.dat', 'r' ) as seedfile:\n",
    "    Reader = csv.reader( seedfile, delimiter='\\t' )\n",
    "    for row in Reader:\n",
    "        T[ row[0].decode('utf-8') ] = row[1:]\n",
    "        print row[0], row[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 begins:\n",
      "Aspect Annotation Done.\n"
     ]
    }
   ],
   "source": [
    "with open( 'review_sentences.pickle', 'r' ) as sentence_file:\n",
    "    [ TripSentences_final ] = pickle.load( sentence_file )\n",
    "    \n",
    "with open( 'Vocabulary.pickle', 'r' ) as Vfile:\n",
    "    [ Vocabulary ] = pickle.load( Vfile )\n",
    "\n",
    "p = 5  # selection threshold\n",
    "max_iter = 10 # iteration step limit\n",
    "tf_cut = 10 # Term frequency filtering\n",
    "\n",
    "iter_num = 0 # Initialize iter_num and keyword list changed.\n",
    "Flag = True\n",
    "\n",
    "Progress = open('RunningProgress.csv', 'a')\n",
    "\n",
    "ProWriter = csv.writer( Progress , delimiter=',' )\n",
    "\n",
    "while iter_num < max_iter and Flag:\n",
    "    Flag = False # set keyword list to unchanged\n",
    "    iter_num += 1\n",
    "    print 'Iteration', iter_num, 'begins:'\n",
    "    \n",
    "    # Match the aspect keywords in each sentence and record its matching\n",
    "    TripSentence_aspects = []\n",
    "    for idx in range( len( TripSentences_final ) ):\n",
    "        sentence_aspects = []\n",
    "        for sentence in TripSentences_final[ idx ]:\n",
    "            aspectcount = {}\n",
    "            aspects = [] # aspects of this sentence\n",
    "            countMax = 0 # maximum count(i)\n",
    "            for key in T.keys():\n",
    "                count = 0\n",
    "                for keyword in T[ key ]:\n",
    "                    count += sentence.count( keyword )\n",
    "                aspectcount[ key ] = count\n",
    "                if count > countMax:\n",
    "                    countMax = count\n",
    "                    \n",
    "            # Assign aspect i\n",
    "            for key in T.keys():\n",
    "                if aspectcount[ key ] == countMax:\n",
    "                    aspects.append( key )\n",
    "            \n",
    "            sentence_aspects.append( aspects )\n",
    "                   \n",
    "        TripSentence_aspects.append( sentence_aspects )\n",
    "    # sentence annotation with aspect assignment done\n",
    "    print 'Aspect Annotation Done.'\n",
    "    \n",
    "    \n",
    "    for key in T.keys():\n",
    "        chi_list = []\n",
    "        count = 0\n",
    "        for word in Vocabulary:\n",
    "            C_1, C_2, C_3, C_4, C = 0, 0, 0, 0, 0\n",
    "            for reviewid in range( len( TripSentences_final ) ): # review level\n",
    "                for idx in range( len( TripSentences_final[ reviewid ] ) ): # sentence level\n",
    "                    C += TripSentences_final[ reviewid ][idx].count( word )\n",
    "                    if key in TripSentence_aspects[ reviewid ][ idx ]:\n",
    "                        C_1 += TripSentences_final[ reviewid ][idx].count( word )\n",
    "                        if not word in TripSentences_final[ reviewid ][ idx ]:\n",
    "                            C_3 += 1\n",
    "                    else:\n",
    "                        C_2 += TripSentences_final[ reviewid ][idx].count( word )\n",
    "                        if not word in TripSentences_final[ reviewid ][ idx ]:\n",
    "                            C_4 += 1\n",
    "            \n",
    "            # Calculate the chi for each word\n",
    "            denominator = (C_1 + C_3) * (C_2 + C_4) * (C_1 + C_2) * (C_3 + C_4)\n",
    "            nominator = C * ( C_1*C_4 - C_2*C_3 ) * ( C_1*C_4 - C_2*C_3 )\n",
    "            if denominator > 0 and C_1 + C_2 > tf_cut:\n",
    "                chi = nominator / float( denominator )\n",
    "            else:\n",
    "                chi = 0.0\n",
    "            chi_list.append( ( word, chi ) )\n",
    "            count += 1\n",
    "            \n",
    "            if count % 1000 == 0:\n",
    "                ProWriter.writerow( [ iter_num,  count, key ] )\n",
    "                \n",
    "        Chi_sorted = sorted( chi_list , key=lambda tup: tup[1] , reverse = True )\n",
    "    \n",
    "        # Joint top p words into aspect keywords list\n",
    "        len_original = len( T[ key ] )\n",
    "        for idx in range( p ):\n",
    "            T[ key ].append( Chi_sorted[ idx ][0] )\n",
    "        T[ key ] = list( set( T[ key ] ) )\n",
    "    \n",
    "        # Test whether the aspect keyword list has changed.\n",
    "        if len( T[ key ] ) > len_original:\n",
    "            Flag = True\n",
    "            \n",
    "Progress.close()\n",
    "            \n",
    "with open('SentenceAspects_Tkey.pickle', 'w') as resultfile:\n",
    "    pickle.dump( [ TripSentence_aspects, T ], resultfile )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Data from Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create $W_{d}$ for each review d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "for idx in range( len( TripAdvisor ) ):\n",
    "    wd = []\n",
    "    for key in T.keys():\n",
    "        wdi = []\n",
    "        for word in Vocabulary:\n",
    "            word_frequency = 0\n",
    "            A_i_total_counts = 0\n",
    "            # word frequency of w_j in text of A_i\n",
    "            for sen_idx in range( len( TripAdvisor[ idx ].sentences_final ) ):\n",
    "                if key in TripAdvisor[ idx ].sentence_aspects[ sen_idx ]:\n",
    "                    word_frequency += TripAdvisor[ idx ].sentences_final[ sen_idx ].count( word )\n",
    "                    A_i_total_counts += len( TripAdvisor[ idx ].sentences_final[ sen_idx ] )\n",
    "            wdi.append( word_frequency / float( A_i_total_counts ) )\n",
    "        wd.append( wdi )\n",
    "    TripAdvisor[ idx ].wd = np.array( wd )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E-step Updating\n",
    "$$L(d) = (\\hat{\\alpha}_d-\\mu)^\\intercal\\Sigma^{-1}(\\hat{\\alpha}_d-\\mu) +\n",
    "\\frac{(r_d - \\alpha_d^\\intercal S_d)^2}{\\delta^2} + \\gamma \\sum_{i=1}^k\n",
    "\\alpha_{di}(S_{di} - r_d )^2$$\n",
    "\n",
    "$$\\frac{\\partial L(d)}{\\partial \\alpha_{di}} =\n",
    "\\frac{2(\\alpha_d^\\intercal S_d - r_d)}{\\delta^2} \\frac{\\partial\n",
    "\\alpha_d^\\intercal S_d}{\\partial\n",
    "\\hat{\\alpha}_{di}} + \\gamma \\frac{\\partial \\sum_{j=1}^k \\alpha_{dj}(S_{dj} -\n",
    "r_d)^2 }{\\partial \\hat{\\alpha}_{di}} + \\frac{\\partial\n",
    "(\\hat{\\alpha}_d-\\mu)^\\intercal\\Sigma^{-1}(\\hat{\\alpha}_d-\\mu)}{\\partial\n",
    "\\hat{\\alpha}_{di}}$$\n",
    "\n",
    "$$\\frac{\\partial \\alpha_d^T S_d}{\\partial \\hat{\\alpha}_{di}} = \\alpha_{di}\n",
    "\\sum_{j=1}^k \\left[ \\tau(j=i)S_{dj}(1-\\alpha_{di}) - \\tau(j\\ne\n",
    "i)S_{dj}\\alpha_{dj} \\right]$$\n",
    "\n",
    "$$\\frac{\\partial \\sum_{j=1}^k \\alpha_{dj}(S_{dj} -\n",
    "r_d)^2}{\\partial \\hat{\\alpha}_{di}} = \\alpha_{di} \\sum_{j=1}^k \\left[\n",
    "\\tau(j=i)(S_{dj}-r_d)^2(1-\\alpha_{di}) - \\tau(j\\ne i)(S_{dj} - r_d)^2\\alpha_{dj}\n",
    "\\right]$$\n",
    "\n",
    "$$\\frac{\\partial\n",
    "(\\hat{\\alpha}_d-\\mu)^\\intercal\\Sigma^{-1}(\\hat{\\alpha}_d-\\mu)}{\\partial\n",
    "\\hat{\\alpha}_{di}} = 2 (\\hat{\\alpha}_d - \\mu) \\Sigma^{-1} \\cdot I \\cdot\n",
    "\\frac{\\partial \\hat{\\alpha}_d}{\\partial \\hat{\\alpha}_{di}} =\n",
    "2\\sum_{j=1}^k \\Sigma_{ji}^{-1}(\\hat{\\alpha}_{dj} - \\mu_j)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from numpy.linalg import inv\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "Theta = [ mu, Sigma, delta_square, beta ]\n",
    "gamma = 0.5\n",
    "\n",
    "# Infer S_d for each review\n",
    "def Estep_sd( review, beta ):\n",
    "    aspectLength = beta.shape[0]\n",
    "    aspect_rating = []\n",
    "    for rowid in range( aspectLength ):\n",
    "        s_di = review.wd[ rowid ].dot( beta[ rowid ].T )\n",
    "        aspect_rating.append( s_di )\n",
    "    return np.array( [ aspect_rating ] ).T\n",
    "\n",
    "# Function L( alphad_hat )\n",
    "def L_alphad_hat( alphad_hat, mu, Sigma_inv, rating, delta_square, Sd, gamma ):\n",
    "    term1 = ( alphad_hat - mu ).T.dot( Sigma_inv ).dot( alphad_hat - mu )\n",
    "    \n",
    "    expsum = np.sum( np.exp( alphad_hat ) )\n",
    "    alphad = np.exp( alphad_hat ) / expsum\n",
    "    term2 = ( rating - alphad.T.dot( Sd ) ) ** 2 / delta_square\n",
    "    \n",
    "    term3 = gamma * alphad.dot( ( Sd - rating ) ** 2 )\n",
    "    return term1 + term2 + term3\n",
    "\n",
    "# Derivative of function L( alphad_hat )\n",
    "def dLdaphad_hat( alphad_hat, mu, Sigma_inv, rating, delta_square, Sd, gamma ):\n",
    "    expsum = np.sum( np.exp( alphad_hat ) )\n",
    "    alphad = np.exp( alphad_hat ) / expsum\n",
    "    \n",
    "    aspectLength = mu.shape[0]\n",
    "    \n",
    "    derivative1 = np.zeros( ( aspectLength , aspectLength ) )\n",
    "    derivative2 = np.zeros( ( aspectLength , aspectLength ) )\n",
    "    for i in range( aspectLength ):\n",
    "        indicator = np.zeros( ( aspectLength ,1 ) )\n",
    "        indicator[ i, 0 ] = 1.0\n",
    "        derivative1[ i, i ] = Sd.T.dot( -alphad + indicator )\n",
    "        derivative2[ i, i ] = ( ( Sd - rating ) ** 2 ).T.dot( -alphad + indicator )\n",
    "    \n",
    "    term1 = 2 * ( alphad.T.dot( Sd ) - rating ) / delta_square * ( derivative1.dot( alphad ) )\n",
    "    \n",
    "    term2 = gamma * ( derivative2.dot( alphad ) )\n",
    "    \n",
    "    term3 = 2 * ( alphad_hat - mu ).T.dot( Sigma_inv ).T\n",
    "    return term1 + term2 + term3\n",
    "\n",
    "# Infer alphad based on LBFGS algorithm\n",
    "def Estep_alphad( alphad_hat0, review, mu, Sigma, delta_square, gamma, gamma ):\n",
    "    \n",
    "    Sigma_inv = inv( Sigma )\n",
    "    \n",
    "    res = minimize( L_alphad_hat, alphad_hat0, \n",
    "                   args=( mu, Sigma_inv, review.rating, delta_square, review.Sd, gamma ),\n",
    "                   method='BFGS', jac=dLdaphad_hat, tol= 1e-2, options={'maxiter':500,'dlsp':True} )\n",
    "    \n",
    "    return res.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M-step Updating\n",
    "$$\n",
    "\\mu_{(t+1)} = \\underset{\\mu}{\\arg\\min} \\sum_{ d\\in D} (\\hat{\\alpha}_d -\n",
    "\\mu)\\Sigma^{-1}(\\hat{\\alpha}_d - \\mu) = \\frac{1}{|D|} \\sum_{d \\in D}\n",
    "\\hat{\\alpha}_d\n",
    "$$\n",
    "$$\n",
    "\\begin{split}\n",
    "\\Sigma_{(t+1)} = & \\underset{\\Sigma}{\\arg\\min} \\sum_{ d\\in D}\n",
    "\\left[ (\\hat{\\alpha}_d - \\mu)\\Sigma^{-1}(\\hat{\\alpha}_d - \\mu) + \\log |\\Sigma|\n",
    "\\right ] \\\\\n",
    "= & \\frac{1}{|D|} \\sum_{d \\in D} (\\hat{\\alpha}_d -\n",
    "\\mu_{(t+1)})^\\intercal (\\hat{\\alpha}_d - \\mu_{(t+1)})\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "\\begin{equation*}\n",
    "L(D) = \\sum_{d \\in D} \\left[ \\log\\delta^2 + \\frac{(r_d -\n",
    "\\alpha_d^TS_d)^2}{\\delta^2} + \\gamma \\sum_{i=1}^k \\alpha_{di}(S_{di} - r_d )^2 \\right] + \\lambda \\beta^\\intercal\n",
    "\\beta\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "\\frac{\\partial L(\\beta)}{\\partial \\beta_i} = & \\sum_{d \\in D} \\left[\n",
    "\\frac{ 2( \\alpha_d^\\intercal S_d - r_d ) }{\\delta^2} \\frac{\\partial\n",
    "\\alpha_d^\\intercal S_d}{\\partial \\beta_i} + 2 \\gamma \\alpha_{di} (S_{di} -\n",
    "r_d) \\frac{\\partial S_{di}}{\\partial \\beta_i} \\right] + 2\\lambda\\beta_i \\\\\n",
    "= & 2 \\sum_{d \\in D} \\alpha_{di} \\left[\n",
    "\\frac{ ( \\alpha_d^\\intercal S_d - r_d ) }{\\delta^2} + \\gamma (S_{di}\n",
    "- r_d) \\right] \\frac{\\partial S_{di}}{\\partial \\beta_i} + 2\\lambda\\beta_i\n",
    "\\end{split}\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "\\delta_{(t+1)}^2 = \\underset{\\delta}{\\arg\\min} \\sum_{d \\in D} \\left[ \\log\n",
    "\\delta^2 + \\frac{(r_d - \\alpha_d^\\intercal S_d)^2}{\\delta^2} \\right] =\n",
    "\\frac{1}{|D|} \\sum_{d \\in D} (r_d - \\alpha_d^\\intercal S_d)^2\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Update mu\n",
    "def Mstep_mu(  TripAdvisor ):\n",
    "    mu = np.zeros_like( TripAdvisor[0].alphad )\n",
    "    for review in TripAdvisor:\n",
    "        mu += review.alphad_hat\n",
    "    mu /= len( TripAdvisor )\n",
    "    return mu\n",
    "\n",
    "# Update Sigma\n",
    "def Mstep_Sigma( TripAdvisor, mu ):\n",
    "    aspectLength = TripAdvisor[0].alphad_hat.shape[0]\n",
    "    Sigma = np.zeros( ( aspectLength, aspectLength ) )\n",
    "    for review in TripAdvisor:\n",
    "        Sigma += ( review.alphad_hat - mu ).T.dot( review.alphad_hat - mu )\n",
    "    Sigma /= len( TripAdvisor )\n",
    "    return Sigma\n",
    "\n",
    "def Mstep_delta_square( TripAdvisor ):\n",
    "    aspectLength = TripAdvisor[0].alphad_hat.shape[0]\n",
    "    delta_square = np.zeros( ( aspectLength, aspectLength ) )\n",
    "    for review in TripAdvisor:\n",
    "        delta_square += ( review.rating - review.alphad.T.dot( review.Sd ) ) ** 2\n",
    "    delta_square /= len( TripAdvisor )\n",
    "    return delta_square\n",
    "\n",
    "def L_beta( beta, TripAdvisor, delta_square, gamma, reg_lambda ):\n",
    "    L = 0\n",
    "    for review in TripAdvisor:\n",
    "        L += ( review.rating - review.alphad.T.dot( review.Sd ) ) ** 2 / delta_square + \n",
    "        gamma * ( review.alphad.T.dot( ( review.Sd - review.rating ) ** 2 ) )\n",
    "    L += reg_lambda * np.trace( beta.T.dot( beta ) )\n",
    "    return\n",
    "\n",
    "def dLdbeta( beta, TripAdvisor, delta_square, gamma, reg_lambda ):\n",
    "    Rows, Cols = TripAdvisor[0].wd.shape\n",
    "    dldbeta = np.zeros( ( Rows, Cols ) )\n",
    "    for review in TripAdvisor:\n",
    "        M_alphad = np.diag( review.alphad.T[0] )\n",
    "        dldbeta += np.diag( ( M_alphad.dot( ( review.alphad.T.dot( review.Sd ) - review.rating ) / delta_square\n",
    "                                + gamma * ( review.Sd - review.rating ) ) ).T[0] ).dot( np.diag( review.Sd.T[0] ).dot( review.wd ) )\n",
    "    dldbeta += reg_lambda * beta\n",
    "    return dldbeta\n",
    "\n",
    "def Mstep_beta( beta0, TripAdvisor, delta_square, gamma, reg_lambda ):\n",
    "    \n",
    "    res = minimize( L_beta, beta0, args=( TripAdvisor, delta_square, gamma, reg_lambda ),\n",
    "                   method='BFGS', jac=dLdbeta, tol= 1e-2, options={'maxiter':5000,'dlsp':True} )\n",
    "    \n",
    "    return res.x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Likelihood estimator\n",
    "1. For each review d infer $s_d$ and $\\alpha_d$ with current $\\Theta_t$\n",
    "2. Based on $r_d$ and $\\alpha_d$ maximization, update $\\Theta_{(t+1)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "import numpy.random.uniform as uniform\n",
    "\n",
    "# Calculate likelihood value of the whole corpus\n",
    "def L_D( TripAdvisor, Sigma, mu, delta_square, gamma, beta, reg_lambda ):\n",
    "    L = 0\n",
    "    for review in TripAdvisor:\n",
    "        L += np.log( numpy.linalg.det( Sigma ) ) + \n",
    "        ( review.alphad_hat - mu ).T.dot( inv( Sigma ) ).dot( review.alphad_hat - mu ) + \n",
    "        np.log( delta_square ) + ( review.rating - review.alphad.T.dot( review.Sd ) ) / delta_square +\n",
    "        gamma *  review.alphad.T.dot( ( review.Sd - review.rating )　** 2 )\n",
    "        \n",
    "    L += reg_lambda * np.sum( beta ** 2 )\n",
    "    return L\n",
    "\n",
    "# Initialize iteration parameters\n",
    "iter_num = 0\n",
    "max_iter = 10\n",
    "convergence = 1e-4\n",
    "diff = 10\n",
    "\n",
    "# Initialize corpus parameters\n",
    "aspectLength = len( T.keys() )\n",
    "vocaLength = len( Vocabulary )\n",
    "\n",
    "Sigma = np.identity( aspectLength )\n",
    "mu = 2 * uniform( size= aspectLength ).reshape( aspectLength, 1 ) - 1.0\n",
    "\n",
    "delta_square = 1.0\n",
    "gamma = 0.5\n",
    "\n",
    "beta = []\n",
    "for i range( aspectLength ):\n",
    "    beta.append( np.random.sample( vocaLength ) )\n",
    "beta = np.array( beta )\n",
    "\n",
    "reg_lambda = 2.0\n",
    "L0 = 1.0\n",
    "\n",
    "\n",
    "while ( diff > convergence and iter_num < max_iter ) or iter_num < min( 8, max_iter ):\n",
    "    \n",
    "    # E-step\n",
    "    for idx in range( len( TripAdvisor ) ):\n",
    "        TripAdvisor[ idx ].Sd = Estep_sd( TripAdvisor[ idx ] , beta )\n",
    "        alphad_hat0 = np.zeros( ( aspectLength, 1 ) )\n",
    "        TripAdvisor[ idx ].alphad_hat = Estep_alphad( alphad_hat0, TripAdvisor[ idx ],\n",
    "                                                     mu, Sigma, delta_square, gamma )\n",
    "        expSum = np.sum( np.exp( TripAdvisor[ idx ].alphad_hat ) )\n",
    "        TripAdvisor[ idx ].alphad = np.exp( TripAdvisor[ idx ].alphad_hat ) / expSum\n",
    "    \n",
    "    # M-step\n",
    "    mu = Mstep_mu(  TripAdvisor )\n",
    "    \n",
    "    # Avoid update Sigma too often\n",
    "    if iter_num % 4 == 3:\n",
    "        Sigma = Mstep_Sigma( TripAdvisor, mu )\n",
    "        \n",
    "    delta_square = Mstep_delta_square( TripAdvisor )\n",
    "    beta = Mstep_beta( beta, TripAdvisor, delta_square, gamma, reg_lambda )\n",
    "    \n",
    "    L1 = L_D( TripAdvisor, Sigma, mu, delta_square, gamma, beta, reg_lambda )\n",
    "    \n",
    "    diff = ( L1 - L0 ) / float( L0 )\n",
    "    \n",
    "    L0 = L1\n",
    "    \n",
    "    iter_num += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "Wang, H., Lu, Y., and Zhai, C. (2010). Latent aspect rating analysis on review text data: a rating regression approach. In Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 783–792. ACM."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
